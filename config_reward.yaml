IO:
  out_dir: out
  eval_interval: 500
  log_interval: 1
  eval_iters: 100
  eval_only: False
  always_save_checkpoint: True
  init_from: resume
  init_multihead_from: scratch
  out_dir_multihead: out_reward
wandb:
  wandb_log: True
  wandb_project: rlhf
  wandb_run_name: gpt2
data:
  dataset: 'openwebtext'
  gradient_accumulation_steps: 1
  batch_size: 64
  block_size: 32
model:
  n_layer: 2
  n_head: 2
  n_embd: 32
  dropout: 0.0
  bias: False
optimizer:
  learning_rate: 6.0e-4
  max_iters: 600000
  weight_decay: 1.0e-2
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0
  decay_lr: True
  warmup_iters: 2000
  lr_decay_iters: 600000
  min_lr: 6.0e-5
DDP:
  backend: nccl
system:
  device: cuda
  dtype: float16
  compile: False
